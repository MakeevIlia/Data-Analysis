{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f326c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import sklearn.metrics as metrics\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.decomposition import KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6dcc04ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_pred):\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_true, y_pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.title('Receiver Operating Characteristic')\n",
    "    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.3f' % roc_auc)\n",
    "    plt.legend(loc = 'lower right')\n",
    "    plt.plot([0, 1], [1, 0], 'r--')\n",
    "    plt.xlim([0, 1])\n",
    "    plt.ylim([0, 1])\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec607f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auc(y_true, y_pred):\n",
    "    fpr, tpr, threshold = metrics.roc_curve(y_true, y_pred)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    return roc_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2ecad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_na(df):\n",
    "    X_train = df.copy()\n",
    "    X_train.dropna(inplace = True)\n",
    "    df_find = df.loc[:, list(set(df.columns))]\n",
    "    targets = []\n",
    "    for i in df_find.columns:\n",
    "        if len(df_find[df_find[i].isna()]) != 0:\n",
    "            targets.append(i)            \n",
    "    for i in targets:\n",
    "        X_train_target = X_train.loc[:, list(set(X_train.columns) - set(targets))]   \n",
    "        Y_train_target = X_train[i]\n",
    "        lr = LinearRegression()\n",
    "        lr.fit(X_train_target, Y_train_target)\n",
    "        df_test = df_find[df_find[i].isna()]\n",
    "        X_test = df_test.loc[:, list(set(df_test.columns) - set(targets))]\n",
    "        X_test = X_test[X_train_target.columns]\n",
    "        Y_test = lr.predict(X_test)\n",
    "        df_find.loc[df_find[i].isna(), i] = Y_test.ravel()\n",
    "#       targets.remove(i)     \n",
    "    df = df_find.copy()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6e9c9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PLS_na(df):\n",
    "    X_train = df.copy()\n",
    "    X_train.dropna(inplace = True)\n",
    "    df_find = df.loc[:, list(set(df.columns))]\n",
    "    targets = []\n",
    "    for i in df_find.columns:\n",
    "        if len(df_find[df_find[i].isna()]) != 0:\n",
    "            targets.append(i)  \n",
    "    current_targets = targets.copy()\n",
    "    \n",
    "    for i in targets:\n",
    "        X_train_target = X_train.loc[:, list(set(X_train.columns) - set(current_targets))]   \n",
    "        Y_train_target = X_train[i]   \n",
    "        pls = PLSRegression(n_components = 5)\n",
    "        pls.fit(X_train_target, Y_train_target)\n",
    "        df_test = df_find[df_find[i].isna()]\n",
    "        X_test = df_test.loc[:, list(set(df_test.columns) - set(current_targets))]\n",
    "        X_test = X_test[X_train_target.columns]\n",
    "        Y_test = pls.predict(X_test)\n",
    "        df_find.loc[df_find[i].isna(), i] = Y_test.ravel()\n",
    "        current_targets.remove(i)     \n",
    "    df = df_find.copy()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4884de47",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'C:/Users/Frederik/Desktop/TrainSample.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "y_train = df[\"INS\"].to_frame()\n",
    "\n",
    "df1 = df\n",
    "cat_vars=['BRANCH','RES'] \n",
    "for var in cat_vars: \n",
    "    cat_list='var'+'_'+var \n",
    "    cat_list = pd.get_dummies(df1[var], prefix=var) \n",
    "    hr1=df1.join(cat_list) \n",
    "    df1=hr1\n",
    "df = df1.loc[:, list(set(df1.columns) - {'BRANCH','RES', \"INS\", \"id\"})].copy()\n",
    "\n",
    "# df = lr_na(df)\n",
    "# df = PLS_na(df)\n",
    "\n",
    "for i in df.columns[df.isnull().any(axis=0)]:\n",
    "    df[i].fillna(df[i].mean(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7dd31867",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path_test = 'C:/Users/Frederik/Desktop/TestSample.csv'\n",
    "\n",
    "df_test = pd.read_csv(file_path_test)\n",
    "\n",
    "y_test = df_test[\"INS\"].to_frame()\n",
    "\n",
    "df1 = df_test\n",
    "cat_vars=['BRANCH','RES'] \n",
    "for var in cat_vars: \n",
    "    cat_list='var'+'_'+var \n",
    "    cat_list = pd.get_dummies(df1[var], prefix=var) \n",
    "    hr1=df1.join(cat_list) \n",
    "    df1=hr1\n",
    "df_test = df1.copy()\n",
    "df_test = df_test.loc[:, list(set(df_test.columns) - {\"BRANCH\", \"RES\", \"id\", \"INS\"})]\n",
    "\n",
    "# df_test = PLS_na(df_test)\n",
    "# df_test = lr_na(df_test)\n",
    "\n",
    "for i in df_test.columns[df_test.isnull().any(axis=0)]:\n",
    "    df_test[i].fillna(df_test[i].mean(),inplace=True)\n",
    "\n",
    "df_test = df_test[df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1aaab1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bin = pd.DataFrame()\n",
    "df_bin_test = pd.DataFrame()\n",
    "\n",
    "df_not_bin = pd.DataFrame()\n",
    "df_not_bin_test = pd.DataFrame()\n",
    "for i in df.columns:\n",
    "    if len(df.loc[df[i] == 0, i]) + len(df.loc[df[i] == 1, i]) == len(df[i]):\n",
    "        df_bin[i] = df[i]\n",
    "        df_bin_test[i] = df_test[i]\n",
    "    else:\n",
    "        df_not_bin[i] = df[i]\n",
    "        df_not_bin_test[i] = df_test[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0168b56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaler = StandardScaler()  \n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(df_not_bin)\n",
    "\n",
    "X_train_not_bin = scaler.transform(df_not_bin)\n",
    "X_test_not_bin = scaler.transform(df_not_bin_test)\n",
    "\n",
    "poly_reg = PolynomialFeatures(degree = 1)\n",
    "X_train = poly_reg.fit_transform(X_train_not_bin) \n",
    "X_test = poly_reg.transform(X_test_not_bin)\n",
    "\n",
    "X_train_bin = df_bin.values\n",
    "X_test_bin = df_bin_test.values\n",
    "\n",
    "X_train_fin = np.concatenate((X_train, X_train_bin), axis=1)\n",
    "X_test_fin = np.concatenate((X_test, X_test_bin), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82dc8456",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.577826\n",
      "         Iterations: 35\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.577826\n",
      "         Iterations: 35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "F:\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.577826\n",
      "         Iterations: 35\n",
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.577826\n",
      "         Iterations: 35\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577826\n",
      "         Iterations 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "F:\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "F:\\anaconda3\\lib\\site-packages\\statsmodels\\base\\model.py:604: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577826\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577826\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577826\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577829\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577829\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577833\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577833\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577837\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577837\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577846\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577846\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577855\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577855\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577865\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577865\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577876\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577876\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577898\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577898\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577921\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577921\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577955\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577955\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577993\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577993\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.578061\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.578061\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.578124\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.578124\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.578197\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.578197\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.578277\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.578277\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.578399\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.578399\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.578517\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.578517\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.578638\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.578638\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.578759\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.578759\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.578910\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.578910\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.579083\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.579083\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.579274\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.579274\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.579465\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.579465\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.579781\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.579781\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.580114\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.580114\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.580455\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.580455\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.580839\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.580839\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.581211\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.581211\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.581704\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.581704\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.582223\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.582223\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.582694\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.582694\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.583182\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.583182\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.583713\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.583713\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.584290\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.584290\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.584932\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.584932\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.585687\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.585687\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.586454\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.586454\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.587272\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.587272\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.588222\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.588222\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.589239\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.589239\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.590311\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.590311\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591720\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.591720\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592897\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.592897\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593983\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.593983\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.595023\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.595023\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.596373\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.596373\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.597909\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.597909\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.599680\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.599680\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.601473\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.601473\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.603374\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.603374\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.605178\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.605178\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.607051\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.607051\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.609052\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.609052\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.611173\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.611173\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.613588\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.613588\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.616637\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.616637\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.619885\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.619885\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.623350\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.623350\n",
      "         Iterations 7\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.626974\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.626974\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.631674\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.631674\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.636595\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.636595\n",
      "         Iterations 6\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.645157\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.645157\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.655523\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.655523\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.668380\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.668380\n",
      "         Iterations 4\n"
     ]
    }
   ],
   "source": [
    "pca_model = PCA()\n",
    "pca_model.fit(X_train_fin)\n",
    "X_train = pca_model.transform(X_train_fin)\n",
    "X_test = pca_model.transform(X_test_fin)\n",
    "\n",
    "best_feats = None\n",
    "features = list(range(X_train.shape[1]))\n",
    "Scores = []\n",
    "\n",
    "while len(features) > 0:\n",
    "    model = sm.Logit(y_train.values.ravel(), X_train[:, features]).fit()\n",
    "    L_score = auc(y_train, model.predict(X_train[:, features]))\n",
    "    Scores.append(L_score)\n",
    "    if max(Scores) == L_score:\n",
    "        best_feats = features.copy()\n",
    "    model = sm.Logit(y_train.values.ravel(), X_train[:, features])\n",
    "    res = model.fit()\n",
    "    worse_feature = features[np.argmax(res.pvalues)]\n",
    "    features.remove(worse_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "faa565ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.577833\n",
      "         Iterations 7\n",
      "0.790915359447866\n",
      "0.7700227686703096\n"
     ]
    }
   ],
   "source": [
    "clf = sm.Logit(y_train.values.ravel(), X_train[:, best_feats]).fit(method = \"newton\")\n",
    "L_score_train = auc(y_train, clf.predict(X_train[:, best_feats]))\n",
    "L_score_test = auc(y_test, clf.predict(X_test[:, best_feats]))\n",
    "\n",
    "print(L_score_train)\n",
    "print(L_score_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a6387057",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>y</td>        <th>  No. Observations:  </th>   <td>  5309</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                 <td>Logit</td>      <th>  Df Residuals:      </th>   <td>  5246</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                 <td>MLE</td>       <th>  Df Model:          </th>   <td>    62</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Sun, 17 Apr 2022</td> <th>  Pseudo R-squ.:     </th>   <td>0.1029</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>21:45:16</td>     <th>  Log-Likelihood:    </th>  <td> -3067.7</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>             <td>True</td>       <th>  LL-Null:           </th>  <td> -3419.5</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>  LLR p-value:       </th> <td>1.621e-109</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "   <td></td>      <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x1</th>  <td>    0.2716</td> <td>    0.051</td> <td>    5.351</td> <td> 0.000</td> <td>    0.172</td> <td>    0.371</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x2</th>  <td>    0.6776</td> <td>    0.055</td> <td>   12.363</td> <td> 0.000</td> <td>    0.570</td> <td>    0.785</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x3</th>  <td>    0.2580</td> <td>    0.058</td> <td>    4.477</td> <td> 0.000</td> <td>    0.145</td> <td>    0.371</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x4</th>  <td>    1.1800</td> <td>    0.066</td> <td>   17.972</td> <td> 0.000</td> <td>    1.051</td> <td>    1.309</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x5</th>  <td>   -0.1928</td> <td>    0.065</td> <td>   -2.968</td> <td> 0.003</td> <td>   -0.320</td> <td>   -0.065</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x6</th>  <td>    0.0835</td> <td>    0.067</td> <td>    1.243</td> <td> 0.214</td> <td>   -0.048</td> <td>    0.215</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x7</th>  <td>    0.2585</td> <td>    0.074</td> <td>    3.483</td> <td> 0.000</td> <td>    0.113</td> <td>    0.404</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x8</th>  <td>   -0.1522</td> <td>    0.077</td> <td>   -1.972</td> <td> 0.049</td> <td>   -0.303</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x9</th>  <td>    1.0263</td> <td>    0.091</td> <td>   11.291</td> <td> 0.000</td> <td>    0.848</td> <td>    1.204</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x10</th> <td>    0.4476</td> <td>    0.092</td> <td>    4.857</td> <td> 0.000</td> <td>    0.267</td> <td>    0.628</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x11</th> <td>    0.2435</td> <td>    0.104</td> <td>    2.337</td> <td> 0.019</td> <td>    0.039</td> <td>    0.448</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x12</th> <td>   -0.5036</td> <td>    0.104</td> <td>   -4.860</td> <td> 0.000</td> <td>   -0.707</td> <td>   -0.300</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x13</th> <td>    0.2615</td> <td>    0.106</td> <td>    2.457</td> <td> 0.014</td> <td>    0.053</td> <td>    0.470</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x14</th> <td>   -0.2600</td> <td>    0.106</td> <td>   -2.451</td> <td> 0.014</td> <td>   -0.468</td> <td>   -0.052</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x15</th> <td>    0.1549</td> <td>    0.109</td> <td>    1.418</td> <td> 0.156</td> <td>   -0.059</td> <td>    0.369</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x16</th> <td>   -0.6892</td> <td>    0.117</td> <td>   -5.914</td> <td> 0.000</td> <td>   -0.918</td> <td>   -0.461</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x17</th> <td>    0.4234</td> <td>    0.115</td> <td>    3.679</td> <td> 0.000</td> <td>    0.198</td> <td>    0.649</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x18</th> <td>    0.0615</td> <td>    0.129</td> <td>    0.477</td> <td> 0.633</td> <td>   -0.191</td> <td>    0.314</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x19</th> <td>    0.9029</td> <td>    0.146</td> <td>    6.204</td> <td> 0.000</td> <td>    0.618</td> <td>    1.188</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x20</th> <td>   -0.7562</td> <td>    0.146</td> <td>   -5.163</td> <td> 0.000</td> <td>   -1.043</td> <td>   -0.469</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x21</th> <td>   -0.6337</td> <td>    0.149</td> <td>   -4.247</td> <td> 0.000</td> <td>   -0.926</td> <td>   -0.341</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x22</th> <td>    0.0864</td> <td>    0.147</td> <td>    0.588</td> <td> 0.557</td> <td>   -0.202</td> <td>    0.375</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x23</th> <td>    0.1330</td> <td>    0.148</td> <td>    0.897</td> <td> 0.370</td> <td>   -0.158</td> <td>    0.424</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x24</th> <td>    0.4072</td> <td>    0.150</td> <td>    2.714</td> <td> 0.007</td> <td>    0.113</td> <td>    0.701</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x25</th> <td>    0.2986</td> <td>    0.163</td> <td>    1.828</td> <td> 0.068</td> <td>   -0.022</td> <td>    0.619</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x26</th> <td>   -0.7887</td> <td>    0.183</td> <td>   -4.321</td> <td> 0.000</td> <td>   -1.146</td> <td>   -0.431</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x27</th> <td>   -0.0567</td> <td>    0.176</td> <td>   -0.322</td> <td> 0.747</td> <td>   -0.402</td> <td>    0.288</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x28</th> <td>   -0.2050</td> <td>    0.178</td> <td>   -1.149</td> <td> 0.251</td> <td>   -0.555</td> <td>    0.145</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x29</th> <td>    0.3412</td> <td>    0.204</td> <td>    1.675</td> <td> 0.094</td> <td>   -0.058</td> <td>    0.740</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x30</th> <td>   -0.1567</td> <td>    0.200</td> <td>   -0.785</td> <td> 0.433</td> <td>   -0.548</td> <td>    0.235</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x31</th> <td>    0.2776</td> <td>    0.198</td> <td>    1.399</td> <td> 0.162</td> <td>   -0.111</td> <td>    0.666</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x32</th> <td>   -0.2497</td> <td>    0.216</td> <td>   -1.158</td> <td> 0.247</td> <td>   -0.672</td> <td>    0.173</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x33</th> <td>   -0.1145</td> <td>    0.225</td> <td>   -0.510</td> <td> 0.610</td> <td>   -0.555</td> <td>    0.326</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x34</th> <td>   -0.0809</td> <td>    0.240</td> <td>   -0.338</td> <td> 0.736</td> <td>   -0.551</td> <td>    0.389</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x35</th> <td>    0.7824</td> <td>    0.270</td> <td>    2.901</td> <td> 0.004</td> <td>    0.254</td> <td>    1.311</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x36</th> <td>   -0.4007</td> <td>    0.280</td> <td>   -1.430</td> <td> 0.153</td> <td>   -0.950</td> <td>    0.149</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x37</th> <td>    1.4605</td> <td>    0.296</td> <td>    4.926</td> <td> 0.000</td> <td>    0.879</td> <td>    2.042</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x38</th> <td>    0.7564</td> <td>    0.309</td> <td>    2.445</td> <td> 0.014</td> <td>    0.150</td> <td>    1.363</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x39</th> <td>   -0.0638</td> <td>    0.298</td> <td>   -0.214</td> <td> 0.830</td> <td>   -0.647</td> <td>    0.519</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x40</th> <td>    0.3801</td> <td>    0.331</td> <td>    1.148</td> <td> 0.251</td> <td>   -0.269</td> <td>    1.029</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x41</th> <td>   -0.2702</td> <td>    0.322</td> <td>   -0.838</td> <td> 0.402</td> <td>   -0.902</td> <td>    0.362</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x42</th> <td>    0.9371</td> <td>    0.362</td> <td>    2.588</td> <td> 0.010</td> <td>    0.228</td> <td>    1.647</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x43</th> <td>    0.8252</td> <td>    0.410</td> <td>    2.013</td> <td> 0.044</td> <td>    0.022</td> <td>    1.629</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x44</th> <td>   -0.7632</td> <td>    0.414</td> <td>   -1.844</td> <td> 0.065</td> <td>   -1.575</td> <td>    0.048</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x45</th> <td>   -2.8694</td> <td>    0.455</td> <td>   -6.306</td> <td> 0.000</td> <td>   -3.761</td> <td>   -1.978</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x46</th> <td>    3.4629</td> <td>    0.492</td> <td>    7.035</td> <td> 0.000</td> <td>    2.498</td> <td>    4.428</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x47</th> <td>   -5.0247</td> <td>    0.582</td> <td>   -8.630</td> <td> 0.000</td> <td>   -6.166</td> <td>   -3.884</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x48</th> <td>   13.8034</td> <td>    1.346</td> <td>   10.257</td> <td> 0.000</td> <td>   11.166</td> <td>   16.441</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x49</th> <td>   -3.6288</td> <td>    0.927</td> <td>   -3.915</td> <td> 0.000</td> <td>   -5.446</td> <td>   -1.812</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x50</th> <td>   -4.9475</td> <td>    0.993</td> <td>   -4.982</td> <td> 0.000</td> <td>   -6.894</td> <td>   -3.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x51</th> <td>   10.3814</td> <td>    1.334</td> <td>    7.782</td> <td> 0.000</td> <td>    7.767</td> <td>   12.996</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x52</th> <td>   -4.2753</td> <td>    1.042</td> <td>   -4.104</td> <td> 0.000</td> <td>   -6.317</td> <td>   -2.233</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x53</th> <td>    5.6577</td> <td>    2.009</td> <td>    2.816</td> <td> 0.005</td> <td>    1.719</td> <td>    9.596</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x54</th> <td>   -7.5295</td> <td>    1.459</td> <td>   -5.162</td> <td> 0.000</td> <td>  -10.388</td> <td>   -4.671</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x55</th> <td>   12.5611</td> <td>    4.104</td> <td>    3.061</td> <td> 0.002</td> <td>    4.517</td> <td>   20.605</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x56</th> <td>   11.0948</td> <td>    2.543</td> <td>    4.363</td> <td> 0.000</td> <td>    6.111</td> <td>   16.079</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x57</th> <td>   -0.5005</td> <td>    1.612</td> <td>   -0.310</td> <td> 0.756</td> <td>   -3.660</td> <td>    2.659</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x58</th> <td>    4.0973</td> <td>    1.830</td> <td>    2.239</td> <td> 0.025</td> <td>    0.511</td> <td>    7.684</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x59</th> <td>    3.6305</td> <td>    4.793</td> <td>    0.757</td> <td> 0.449</td> <td>   -5.764</td> <td>   13.025</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x60</th> <td>   -2.1263</td> <td>    3.050</td> <td>   -0.697</td> <td> 0.486</td> <td>   -8.103</td> <td>    3.851</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x61</th> <td>   10.3227</td> <td>    3.322</td> <td>    3.107</td> <td> 0.002</td> <td>    3.811</td> <td>   16.834</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x62</th> <td>    3.9785</td> <td>    4.451</td> <td>    0.894</td> <td> 0.371</td> <td>   -4.746</td> <td>   12.703</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>x63</th> <td>   -1.1953</td> <td>    3.945</td> <td>   -0.303</td> <td> 0.762</td> <td>   -8.927</td> <td>    6.536</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                      y   No. Observations:                 5309\n",
       "Model:                          Logit   Df Residuals:                     5246\n",
       "Method:                           MLE   Df Model:                           62\n",
       "Date:                Sun, 17 Apr 2022   Pseudo R-squ.:                  0.1029\n",
       "Time:                        21:45:16   Log-Likelihood:                -3067.7\n",
       "converged:                       True   LL-Null:                       -3419.5\n",
       "Covariance Type:            nonrobust   LLR p-value:                1.621e-109\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "x1             0.2716      0.051      5.351      0.000       0.172       0.371\n",
       "x2             0.6776      0.055     12.363      0.000       0.570       0.785\n",
       "x3             0.2580      0.058      4.477      0.000       0.145       0.371\n",
       "x4             1.1800      0.066     17.972      0.000       1.051       1.309\n",
       "x5            -0.1928      0.065     -2.968      0.003      -0.320      -0.065\n",
       "x6             0.0835      0.067      1.243      0.214      -0.048       0.215\n",
       "x7             0.2585      0.074      3.483      0.000       0.113       0.404\n",
       "x8            -0.1522      0.077     -1.972      0.049      -0.303      -0.001\n",
       "x9             1.0263      0.091     11.291      0.000       0.848       1.204\n",
       "x10            0.4476      0.092      4.857      0.000       0.267       0.628\n",
       "x11            0.2435      0.104      2.337      0.019       0.039       0.448\n",
       "x12           -0.5036      0.104     -4.860      0.000      -0.707      -0.300\n",
       "x13            0.2615      0.106      2.457      0.014       0.053       0.470\n",
       "x14           -0.2600      0.106     -2.451      0.014      -0.468      -0.052\n",
       "x15            0.1549      0.109      1.418      0.156      -0.059       0.369\n",
       "x16           -0.6892      0.117     -5.914      0.000      -0.918      -0.461\n",
       "x17            0.4234      0.115      3.679      0.000       0.198       0.649\n",
       "x18            0.0615      0.129      0.477      0.633      -0.191       0.314\n",
       "x19            0.9029      0.146      6.204      0.000       0.618       1.188\n",
       "x20           -0.7562      0.146     -5.163      0.000      -1.043      -0.469\n",
       "x21           -0.6337      0.149     -4.247      0.000      -0.926      -0.341\n",
       "x22            0.0864      0.147      0.588      0.557      -0.202       0.375\n",
       "x23            0.1330      0.148      0.897      0.370      -0.158       0.424\n",
       "x24            0.4072      0.150      2.714      0.007       0.113       0.701\n",
       "x25            0.2986      0.163      1.828      0.068      -0.022       0.619\n",
       "x26           -0.7887      0.183     -4.321      0.000      -1.146      -0.431\n",
       "x27           -0.0567      0.176     -0.322      0.747      -0.402       0.288\n",
       "x28           -0.2050      0.178     -1.149      0.251      -0.555       0.145\n",
       "x29            0.3412      0.204      1.675      0.094      -0.058       0.740\n",
       "x30           -0.1567      0.200     -0.785      0.433      -0.548       0.235\n",
       "x31            0.2776      0.198      1.399      0.162      -0.111       0.666\n",
       "x32           -0.2497      0.216     -1.158      0.247      -0.672       0.173\n",
       "x33           -0.1145      0.225     -0.510      0.610      -0.555       0.326\n",
       "x34           -0.0809      0.240     -0.338      0.736      -0.551       0.389\n",
       "x35            0.7824      0.270      2.901      0.004       0.254       1.311\n",
       "x36           -0.4007      0.280     -1.430      0.153      -0.950       0.149\n",
       "x37            1.4605      0.296      4.926      0.000       0.879       2.042\n",
       "x38            0.7564      0.309      2.445      0.014       0.150       1.363\n",
       "x39           -0.0638      0.298     -0.214      0.830      -0.647       0.519\n",
       "x40            0.3801      0.331      1.148      0.251      -0.269       1.029\n",
       "x41           -0.2702      0.322     -0.838      0.402      -0.902       0.362\n",
       "x42            0.9371      0.362      2.588      0.010       0.228       1.647\n",
       "x43            0.8252      0.410      2.013      0.044       0.022       1.629\n",
       "x44           -0.7632      0.414     -1.844      0.065      -1.575       0.048\n",
       "x45           -2.8694      0.455     -6.306      0.000      -3.761      -1.978\n",
       "x46            3.4629      0.492      7.035      0.000       2.498       4.428\n",
       "x47           -5.0247      0.582     -8.630      0.000      -6.166      -3.884\n",
       "x48           13.8034      1.346     10.257      0.000      11.166      16.441\n",
       "x49           -3.6288      0.927     -3.915      0.000      -5.446      -1.812\n",
       "x50           -4.9475      0.993     -4.982      0.000      -6.894      -3.001\n",
       "x51           10.3814      1.334      7.782      0.000       7.767      12.996\n",
       "x52           -4.2753      1.042     -4.104      0.000      -6.317      -2.233\n",
       "x53            5.6577      2.009      2.816      0.005       1.719       9.596\n",
       "x54           -7.5295      1.459     -5.162      0.000     -10.388      -4.671\n",
       "x55           12.5611      4.104      3.061      0.002       4.517      20.605\n",
       "x56           11.0948      2.543      4.363      0.000       6.111      16.079\n",
       "x57           -0.5005      1.612     -0.310      0.756      -3.660       2.659\n",
       "x58            4.0973      1.830      2.239      0.025       0.511       7.684\n",
       "x59            3.6305      4.793      0.757      0.449      -5.764      13.025\n",
       "x60           -2.1263      3.050     -0.697      0.486      -8.103       3.851\n",
       "x61           10.3227      3.322      3.107      0.002       3.811      16.834\n",
       "x62            3.9785      4.451      0.894      0.371      -4.746      12.703\n",
       "x63           -1.1953      3.945     -0.303      0.762      -8.927       6.536\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
